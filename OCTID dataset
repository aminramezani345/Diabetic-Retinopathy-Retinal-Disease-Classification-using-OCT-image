"""ROCT-Net_OCTID dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/mr7495/OCT-classification/blob/main/ROCT_Net_OCTID_dataset.ipynb
"""

!pip install --upgrade tensorflow==2.13
!pip install tensorflow_addons
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
import cv2
import zipfile
import shutil
import random
import pandas as pd
import csv
import os
import zipfile
import os
import tensorflow as tf
from keras_contrib.layers import Capsule
from keras_contrib.activations import squash
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.python.keras.engine.keras_tensor import KerasTensor
import sys
import os

archive = zipfile.ZipFile(r'C:\Users\u249391\Downloads\OCT-classification-main\OCTA-gholami.zip')
for file in archive.namelist():
     archive.extract(file, r'C:/Users/u249391/Downloads/OCT-classification-main')
extraction_path=r"C:\Users\u249391\Downloads\OCT-classification-main\gholami"


train_datagen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,vertical_flip=True
                                                          ,zoom_range=0.1,rotation_range=360
                                                             ,width_shift_range=0.1,height_shift_range=0.1)
test_datagen = keras.preprocessing.image.ImageDataGenerator()

train_df = pd.read_csv(r"C:\Users\u249391\Downloads\OCT-classification-main\train_gholami.csv")
test_df = pd.read_csv(r"C:\Users\u249391\Downloads\OCT-classification-main\test_gholami.csv")

#create dataloader
shape=(512, 512)
batch_size=8 #increase when having better GPU
train_generator = train_datagen.flow_from_dataframe(
      dataframe=train_df,
      directory=r'C:\Users\u249391\Downloads\OCT-classification-main\gholami',
      x_col="filename",
      y_col="class",
      target_size=shape,
      batch_size=batch_size,
      class_mode='categorical',shuffle=True)
validation_generator = test_datagen.flow_from_dataframe(
        dataframe=test_df,
        directory=r'C:\Users\u249391\Downloads\OCT-classification-main\gholami',
        x_col="filename",
        y_col="class",
        target_size=shape,
        batch_size=batch_size,
        class_mode='categorical',shuffle=True)
train_img_num=len(train_generator.filenames)

sys.path.append(r"C:\Users\u249391\Downloads\OCT-classification-main\automl-master\efficientnetv2")
from effnetv2_model import get_model

name="ROCT-Net_OCTID dataset"
!mkdir "models"
keras.backend.clear_session()
input_tensor=keras.Input(shape=(shape[0],shape[1],3))
base_model1=get_model('efficientnetv2-b0', include_top=False, pretrained=True)(input_tensor) #load EfficientNetV2-B0
base_model2=keras.applications.Xception(input_tensor=input_tensor,weights='imagenet',include_top=False)(input_tensor) #load Xception

concatenated=keras.layers.concatenate([base_model1,base_model2]) #load concatenated model

avg=keras.layers.AveragePooling2D(3,padding='valid')(concatenated) #deploy Wise-srNet
depthw=keras.layers.DepthwiseConv2D(5,
                                      depthwise_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),
                                      bias_initializer=keras.initializers.Zeros(),depthwise_constraint=keras.constraints.NonNeg())(avg)
# Define Capsules
capsule = Capsule(num_capsule=10,
                dim_capsule=16,
                routings=3,
                activation=squash,
                share_weights=True)(depthw)

flat=keras.layers.Flatten()(capsule)
dp=keras.layers.Dropout(0.2)(flat)
preds=keras.layers.Dense(5,activation='softmax',
                          kernel_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),
                          bias_initializer=keras.initializers.Zeros(),)(dp)
model=keras.Model(inputs=input_tensor, outputs=preds)  

##################################
for layer in model.layers:
  layer.trainable = True
model.summary()
filepath="models/%s-{epoch:02d}-{val_accuracy:.4f}.hdf5"%name

#Note that keras classic saving function would not save capsule weights, so the saved model will not be as equal as the trained model. This bug must be fixed in the future.

checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max',save_weights_only=True) #creating checkpoint to save the best validation accuracy
callbacks_list = [checkpoint]

lr_schedule =keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.045,
    decay_steps=2*int(train_img_num/batch_size),
    decay_rate=0.94,
    staircase=True)
optimizer=keras.optimizers.SGD(momentum=0.9,learning_rate=lr_schedule)
model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])

hist=model.fit_generator(train_generator, epochs=1,validation_data=validation_generator,shuffle=True,callbacks=callbacks_list) #start training
with open('{}-results.csv'.format(name), mode='w',newline='') as csv_file: #write evaluation metrics
  csv_writer = csv.writer(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
  for key in hist.history:
    data=[key]
    data.extend(hist.history[key])
    csv_writer.writerow(data)
    breakpoint 
